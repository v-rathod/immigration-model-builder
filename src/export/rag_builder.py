"""
RAG Builder — Generate retrieval-augmented-generation chunks from Meridian artifacts.

Produces JSON chunks that Compass (P3) can load directly into an LLM context window.
No vector DB required — chunks are pre-tagged with topic/entity metadata for
keyword-based retrieval, with optional embeddings for semantic search.

Output directory: artifacts/rag/
  - catalog.json      — full artifact registry (system prompt material)
  - chunks/           — per-topic JSON chunk files
  - qa_cache.json     — pre-computed Q&A pairs (generated by qa_generator.py)
  - embeddings.npz    — optional numpy vectors (generated separately)

Design principles:
  - Pre-compute everything so P3 does zero Parquet reads at runtime
  - Chunk size targets ~500-800 tokens (fits in GPT-4o-mini context easily)
  - Each chunk is self-contained with source attribution
  - Metadata enables topic filtering before LLM call (reduces token cost)
"""

import json
import hashlib
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import pandas as pd
import pyarrow.parquet as pq


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

ARTIFACTS_ROOT = Path("artifacts/tables")
RAG_ROOT = Path("artifacts/rag")
CHUNKS_DIR = RAG_ROOT / "chunks"

# Max rows to include in sample data chunks
SAMPLE_ROWS = 10

# Categories for chunk tagging
TOPICS = {
    "pd_forecast": ["pd_forecasts", "fact_cutoff_trends", "category_movement_metrics",
                     "backlog_estimates", "fact_cutoffs_all", "queue_depth_estimates"],
    "employer": ["employer_friendliness_scores", "employer_friendliness_scores_ml",
                 "employer_features", "employer_monthly_metrics", "employer_risk_features",
                 "dim_employer", "fact_h1b_employer_hub"],
    "salary": ["salary_benchmarks", "fact_oews", "fact_bls_ces"],
    "visa_bulletin": ["fact_cutoffs_all", "fact_cutoff_trends", "category_movement_metrics",
                      "dim_visa_class", "dim_visa_ceiling", "fact_cutoffs",
                      "fact_waiting_list"],
    "geographic": ["worksite_geo_metrics", "dim_area"],
    "occupation": ["soc_demand_metrics", "dim_soc", "fact_lca"],
    "processing": ["processing_times_trends", "fact_uscis_approvals"],
    "visa_demand": ["visa_demand_metrics", "fact_visa_issuance",
                    "fact_visa_applications", "fact_niv_issuance",
                    "fact_iv_post"],
    "filings": ["fact_lca", "fact_perm", "fact_perm_unique_case"],
    "general": ["dim_country", "fact_dhs_admissions", "fact_warn_events"],
}


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _chunk_id(source: str, label: str) -> str:
    """Deterministic chunk ID from source artifact + label."""
    raw = f"{source}::{label}"
    return hashlib.sha256(raw.encode()).hexdigest()[:12]


def _safe_read(path: Path) -> pd.DataFrame | None:
    """Read a parquet file/directory, returning None on failure."""
    try:
        if path.is_dir():
            return pd.read_parquet(path)
        elif path.exists():
            return pd.read_parquet(path)
    except Exception:
        # Handle schema merge errors (e.g. fact_lca)
        if path.is_dir():
            # Try reading individual partition files
            parts = sorted(path.glob("*.parquet"))
            if parts:
                try:
                    return pd.read_parquet(parts[0])
                except Exception:
                    pass
    return None


def _table_stats(df: pd.DataFrame) -> dict:
    """Compute basic statistics for a dataframe."""
    stats = {
        "rows": len(df),
        "columns": len(df.columns),
        "column_names": list(df.columns),
    }
    # Add numeric column stats
    num_cols = df.select_dtypes(include="number").columns.tolist()
    if num_cols:
        desc = df[num_cols].describe().round(2)
        stats["numeric_summary"] = {
            col: {
                "min": desc.loc["min", col] if "min" in desc.index else None,
                "max": desc.loc["max", col] if "max" in desc.index else None,
                "mean": desc.loc["mean", col] if "mean" in desc.index else None,
            }
            for col in num_cols[:8]  # Limit to 8 columns for chunk size
        }
    return stats


def _make_chunk(source: str, label: str, topic: str, text: str,
                metadata: dict | None = None) -> dict:
    """Create a standardized chunk dict."""
    return {
        "chunk_id": _chunk_id(source, label),
        "source_artifact": source,
        "topic": topic,
        "label": label,
        "text": text,
        "metadata": metadata or {},
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }


# ---------------------------------------------------------------------------
# Chunk generators
# ---------------------------------------------------------------------------

def _build_catalog_chunk() -> dict:
    """System-level catalog describing all available artifacts."""
    artifacts = []
    for p in sorted(ARTIFACTS_ROOT.iterdir()):
        if p.name.startswith(".") or p.name.startswith("_"):
            continue
        df = _safe_read(p)
        if df is not None:
            artifacts.append({
                "name": p.name,
                "rows": len(df),
                "columns": list(df.columns),
                "is_partitioned": p.is_dir(),
            })
        elif p.is_dir():
            parts = list(p.glob("*.parquet"))
            artifacts.append({
                "name": p.name,
                "partitions": len(parts),
                "is_partitioned": True,
                "note": "schema merge error — read individual partitions",
            })
    return artifacts


def _build_pd_forecast_chunks(chunks: list) -> None:
    """Generate chunks for priority date forecasts."""
    path = ARTIFACTS_ROOT / "pd_forecasts.parquet"
    df = _safe_read(path)
    if df is None:
        return

    # Overall summary chunk
    categories = df["category"].unique().tolist() if "category" in df.columns else []
    countries = df["country"].unique().tolist() if "country" in df.columns else []

    summary = (
        f"Priority Date Forecast Summary (NorthStar Meridian v2.1)\n"
        f"Total forecast series: {len(df)} rows covering {len(set(categories))} categories "
        f"and {len(set(countries))} countries.\n"
        f"Categories: {', '.join(sorted(set(categories)))}\n"
        f"Countries: {', '.join(sorted(set(countries)))}\n"
        f"Forecast horizon: 24 months from latest visa bulletin.\n"
        f"Model: Exponential-weighted seasonal with velocity cap. "
        f"Cross-verified within ±18% of 10-year actual velocity.\n"
        f"Velocity blend: 50% full-history net + 25% capped 24-month + 25% capped 12-month."
    )
    chunks.append(_make_chunk("pd_forecasts.parquet", "pd_forecast_summary",
                              "pd_forecast", summary,
                              {"categories": categories, "countries": countries}))

    # Per-category × country chunks
    if "category" in df.columns and "country" in df.columns:
        for (cat, cty), grp in df.groupby(["category", "country"]):
            sample = grp.head(SAMPLE_ROWS)
            # Build a text description
            cols_to_show = [c for c in sample.columns
                            if c not in ("category", "country")]
            lines = [f"PD Forecast — {cat} / {cty} ({len(grp)} months):"]
            for _, row in sample.iterrows():
                parts = [f"{c}={row[c]}" for c in cols_to_show if pd.notna(row[c])]
                lines.append("  " + ", ".join(parts))
            if len(grp) > SAMPLE_ROWS:
                lines.append(f"  ... and {len(grp) - SAMPLE_ROWS} more months")

            text = "\n".join(lines)
            chunks.append(_make_chunk("pd_forecasts.parquet",
                                      f"pd_forecast_{cat}_{cty}",
                                      "pd_forecast", text,
                                      {"category": cat, "country": cty}))


def _build_employer_chunks(chunks: list) -> None:
    """Generate chunks for employer scores and features."""
    # EFS rules-based
    efs_path = ARTIFACTS_ROOT / "employer_friendliness_scores.parquet"
    df = _safe_read(efs_path)
    if df is None:
        return

    # Summary
    tier_col = None
    for c in ["tier", "efs_tier", "friendliness_tier"]:
        if c in df.columns:
            tier_col = c
            break

    score_col = None
    for c in ["efs", "efs_score", "friendliness_score", "score"]:
        if c in df.columns:
            score_col = c
            break

    summary_lines = [
        f"Employer Friendliness Score (EFS) Summary:",
        f"Total employers scored: {len(df):,}",
        f"Score range: 0–100 (higher is better)",
        f"Scoring formula: 50% approval rate (Bayesian-shrunk) + 30% wage ratio + 20% sustainability",
        f"Tiers: Excellent ≥85, Good ≥70, Moderate ≥50, Below Average ≥30, Poor <30",
    ]
    if tier_col:
        tier_counts = df[tier_col].value_counts().to_dict()
        summary_lines.append(f"Tier distribution: {tier_counts}")
    if score_col:
        summary_lines.append(
            f"Score stats: mean={df[score_col].mean():.1f}, "
            f"median={df[score_col].median():.1f}, "
            f"std={df[score_col].std():.1f}"
        )

    chunks.append(_make_chunk("employer_friendliness_scores.parquet",
                              "efs_summary", "employer",
                              "\n".join(summary_lines)))

    # Top employers chunk
    if score_col and "employer_name" in df.columns:
        rated = df.dropna(subset=[score_col])
        top = rated.nlargest(50, score_col)
        lines = ["Top 50 Employers by EFS Score:"]
        for _, row in top.iterrows():
            name = row["employer_name"]
            score = row[score_col]
            tier = row[tier_col] if tier_col else ""
            lines.append(f"  {name}: {score:.1f} ({tier})")
        chunks.append(_make_chunk("employer_friendliness_scores.parquet",
                                  "efs_top50", "employer",
                                  "\n".join(lines)))

    # Bottom employers chunk (risk signals)
    if score_col and "employer_name" in df.columns:
        rated = df.dropna(subset=[score_col])
        # Only employers with meaningful volume
        vol_col = None
        for c in ["n_24m", "n_36m", "total_cases", "case_count", "n_cases"]:
            if c in rated.columns:
                vol_col = c
                break
        if vol_col:
            active = rated[rated[vol_col] >= 10]
        else:
            active = rated
        bottom = active.nsmallest(30, score_col)
        lines = ["Bottom 30 Employers by EFS Score (≥10 cases):"]
        for _, row in bottom.iterrows():
            name = row["employer_name"]
            score = row[score_col]
            tier = row[tier_col] if tier_col else ""
            lines.append(f"  {name}: {score:.1f} ({tier})")
        chunks.append(_make_chunk("employer_friendliness_scores.parquet",
                                  "efs_bottom30", "employer",
                                  "\n".join(lines)))


def _build_salary_chunks(chunks: list) -> None:
    """Generate chunks for salary benchmarks."""
    path = ARTIFACTS_ROOT / "salary_benchmarks.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"Salary Benchmarks Summary:\n"
        f"Total records: {len(df):,} (SOC × area combinations)\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Source: BLS OEWS 2022–2024 + PERM wage data\n"
        f"Use: Compare offered wages against prevailing wage percentiles "
        f"(P10, P25, median, P75, P90) for specific occupation + geography."
    )
    chunks.append(_make_chunk("salary_benchmarks.parquet",
                              "salary_summary", "salary", summary,
                              _table_stats(df)))

    # ── Top SOC codes by median salary ────────────────────────────────────
    if "soc_code" in df.columns and "median" in df.columns:
        # National-level medians (area_code ~= "1" or aggregate)
        national = df.copy()
        if "area_code" in df.columns:
            nat_mask = df["area_code"].astype(str).isin(["1", "0", "C0000"])
            if nat_mask.sum() > 50:
                national = df[nat_mask]

        # Look up SOC titles
        dim_soc_path = ARTIFACTS_ROOT / "dim_soc.parquet"
        soc_lookup: dict[str, str] = {}
        dim_soc = _safe_read(dim_soc_path)
        if dim_soc is not None:
            for col in ["soc_title", "title", "occupation_title"]:
                if col in dim_soc.columns and "soc_code" in dim_soc.columns:
                    soc_lookup = dict(zip(dim_soc["soc_code"], dim_soc[col]))
                    break

        top_paid = (
            national.dropna(subset=["median"])
            .drop_duplicates(subset=["soc_code"])
            .nlargest(30, "median")
        )
        lines = ["Top 30 Highest-Paying Occupations (national median):"]
        for _, row in top_paid.iterrows():
            code = row["soc_code"]
            title = soc_lookup.get(code, "")
            med = row["median"]
            p75 = row.get("p75", "")
            lines.append(f"  {code} ({title}): median=${med:,.0f}, P75=${p75:,.0f}" if pd.notna(p75) else f"  {code} ({title}): median=${med:,.0f}")
        chunks.append(_make_chunk("salary_benchmarks.parquet",
                                  "salary_top30_soc", "salary",
                                  "\n".join(lines)))

        # Tech / immigration-heavy SOC codes
        tech_socs = {
            "15-1252": "Software Developers",
            "15-1256": "Software Developers & Programmers",
            "15-1211": "Computer Systems Analysts",
            "15-1299": "Computer Occupations, All Other",
            "15-1241": "Computer Network Architects",
            "15-1244": "Network & Systems Administrators",
            "15-1232": "Computer User Support",
            "11-3021": "Computer & IS Managers",
            "15-2051": "Data Scientists",
            "17-2061": "Computer Hardware Engineers",
            "15-1221": "Computer & Info Research Scientists",
        }
        tech_rows = national[national["soc_code"].isin(tech_socs.keys())]
        if len(tech_rows) > 0:
            lines = ["Salary Benchmarks for Common Immigration-Sponsored Tech Roles:"]
            for _, row in tech_rows.iterrows():
                code = row["soc_code"]
                title = tech_socs.get(code, soc_lookup.get(code, ""))
                med = row["median"]
                p10 = row.get("p10", "")
                p90 = row.get("p90", "")
                parts = [f"{code} ({title}): median=${med:,.0f}"]
                if pd.notna(p10):
                    parts.append(f"P10=${p10:,.0f}")
                if pd.notna(p90):
                    parts.append(f"P90=${p90:,.0f}")
                lines.append("  " + ", ".join(parts))
            chunks.append(_make_chunk("salary_benchmarks.parquet",
                                      "salary_tech_socs", "salary",
                                      "\n".join(lines)))

    # ── Salary range distribution ─────────────────────────────────────────
    if "median" in df.columns:
        valid = df["median"].dropna()
        lines = [
            "Salary Distribution Across All SOC × Area Combinations:",
            f"  Total records: {len(valid):,}",
            f"  Min median salary: ${valid.min():,.0f}",
            f"  25th percentile: ${valid.quantile(0.25):,.0f}",
            f"  Overall median: ${valid.median():,.0f}",
            f"  75th percentile: ${valid.quantile(0.75):,.0f}",
            f"  Max median salary: ${valid.max():,.0f}",
            f"  Wage ratio interpretation: ratio ≥1.0 means offered wage ≥ prevailing median.",
            f"  Employers with wage ratio ≥1.3 (P75+) tend to have higher EFS scores.",
        ]
        chunks.append(_make_chunk("salary_benchmarks.parquet",
                                  "salary_distribution", "salary",
                                  "\n".join(lines)))


def _build_geo_chunks(chunks: list) -> None:
    """Generate chunks for geographic/worksite metrics."""
    path = ARTIFACTS_ROOT / "worksite_geo_metrics.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary_lines = [
        f"Worksite Geographic Metrics Summary:",
        f"Total records: {len(df):,} (multiple geographic grains)",
        f"Columns: {', '.join(df.columns.tolist())}",
        f"Grains: state, metro area, county, city",
        f"Source: PERM + LCA filings cross-referenced with OEWS wages",
    ]

    # Top states by filings
    if "state" in df.columns and "filings_count" in df.columns:
        state_df = df.groupby("state")["filings_count"].sum().nlargest(15)
        summary_lines.append("\nTop 15 states by sponsorship filings:")
        for state, count in state_df.items():
            summary_lines.append(f"  {state}: {count:,.0f}")

    chunks.append(_make_chunk("worksite_geo_metrics.parquet",
                              "geo_summary", "geographic",
                              "\n".join(summary_lines)))

    # ── Per-state detail chunks ───────────────────────────────────────────
    if "grain" in df.columns and "state" in df.columns:
        state_data = df[df["grain"] == "state"].copy()
        if len(state_data) > 0:
            agg_cols = {}
            for col in ["filings_count", "approvals_count", "distinct_employers",
                        "offered_median", "competitiveness_ratio"]:
                if col in state_data.columns:
                    agg_cols[col] = "sum" if col in ("filings_count", "approvals_count") else "mean"

            if agg_cols:
                by_state = (
                    state_data.groupby("state")
                    .agg(agg_cols)
                    .sort_values("filings_count", ascending=False)
                )
                lines = ["State-Level Sponsorship Summary (all datasets):"]
                for state, row in by_state.head(25).iterrows():
                    parts = [f"{state}:"]
                    if "filings_count" in row.index:
                        parts.append(f"filings={row['filings_count']:,.0f}")
                    if "approvals_count" in row.index and pd.notna(row["approvals_count"]):
                        parts.append(f"approvals={row['approvals_count']:,.0f}")
                    if "distinct_employers" in row.index and pd.notna(row["distinct_employers"]):
                        parts.append(f"employers={row['distinct_employers']:,.0f}")
                    if "offered_median" in row.index and pd.notna(row["offered_median"]):
                        parts.append(f"median_wage=${row['offered_median']:,.0f}")
                    lines.append("  " + " ".join(parts))
                chunks.append(_make_chunk("worksite_geo_metrics.parquet",
                                          "geo_state_detail", "geographic",
                                          "\n".join(lines)))

    # ── Top cities ────────────────────────────────────────────────────────
    if "grain" in df.columns and "city" in df.columns:
        city_data = df[df["grain"] == "city"].copy()
        if len(city_data) > 0 and "filings_count" in city_data.columns:
            top_cities = city_data.nlargest(30, "filings_count")
            lines = ["Top 30 Cities by Immigration Sponsorship Filings:"]
            for _, row in top_cities.iterrows():
                city = row["city"] if pd.notna(row.get("city")) else "Unknown"
                state = row.get("state", "")
                filings = row["filings_count"]
                parts = [f"{city}, {state}: {filings:,.0f} filings"]
                if "offered_median" in row.index and pd.notna(row["offered_median"]):
                    parts.append(f"median=${row['offered_median']:,.0f}")
                if "distinct_employers" in row.index and pd.notna(row["distinct_employers"]):
                    parts.append(f"{row['distinct_employers']:,.0f} employers")
                lines.append("  " + ", ".join(parts))
            chunks.append(_make_chunk("worksite_geo_metrics.parquet",
                                      "geo_top_cities", "geographic",
                                      "\n".join(lines)))

    # ── Competitiveness by area ───────────────────────────────────────────
    if "competitiveness_ratio" in df.columns and "grain" in df.columns:
        area_data = df[df["grain"].isin(["state", "area"])].copy()
        cr = area_data["competitiveness_ratio"].dropna()
        if len(cr) > 0:
            lines = [
                "Geographic Competitiveness Analysis:",
                f"  Competitiveness ratio = filings per employer (higher = more competitive).",
                f"  Overall: mean={cr.mean():.1f}, median={cr.median():.1f}, "
                f"max={cr.max():.1f}",
                f"  Areas with ratio >2.0 indicate intense competition for sponsorship.",
                f"  Areas with ratio <1.0 indicate distributed sponsorship across many employers.",
            ]
            chunks.append(_make_chunk("worksite_geo_metrics.parquet",
                                      "geo_competitiveness", "geographic",
                                      "\n".join(lines)))


def _build_occupation_chunks(chunks: list) -> None:
    """Generate chunks for SOC/occupation demand."""
    path = ARTIFACTS_ROOT / "soc_demand_metrics.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"SOC Occupation Demand Metrics Summary:\n"
        f"Total records: {len(df):,}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Windows: 12-month, 24-month, 36-month rolling\n"
        f"Sources: PERM + LCA filing data\n"
        f"Use: Identify high-demand occupations, filing trends, and approval rates by SOC code."
    )
    chunks.append(_make_chunk("soc_demand_metrics.parquet",
                              "soc_demand_summary", "occupation", summary,
                              _table_stats(df)))

    # Top SOCs chunk
    if "soc_code" in df.columns and "filings_count" in df.columns:
        # Get SOC descriptions from dim_soc
        dim_soc_path = ARTIFACTS_ROOT / "dim_soc.parquet"
        soc_lookup = {}
        dim_df = _safe_read(dim_soc_path)
        if dim_df is not None:
            title_col = None
            for c in ["soc_title", "title", "occupation_title"]:
                if c in dim_df.columns:
                    title_col = c
                    break
            if title_col and "soc_code" in dim_df.columns:
                soc_lookup = dict(zip(dim_df["soc_code"], dim_df[title_col]))

        top = df.nlargest(30, "filings_count")
        lines = ["Top 30 SOC Codes by Sponsorship Filing Volume:"]
        for _, row in top.iterrows():
            code = row["soc_code"]
            count = row["filings_count"]
            title = soc_lookup.get(code, "")
            lines.append(f"  {code} ({title}): {count:,.0f} filings")

        chunks.append(_make_chunk("soc_demand_metrics.parquet",
                                  "soc_top30", "occupation",
                                  "\n".join(lines)))


def _build_processing_chunks(chunks: list) -> None:
    """Generate chunks for processing times."""
    path = ARTIFACTS_ROOT / "processing_times_trends.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"USCIS Processing Times Trends:\n"
        f"Total records: {len(df)}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Coverage: FY2014–FY2025 quarterly USCIS I-485 performance data\n"
        f"Use: Track processing speed changes, estimate case timelines."
    )
    chunks.append(_make_chunk("processing_times_trends.parquet",
                              "processing_summary", "processing", summary))

    # ── Detailed FY-by-FY table ──────────────────────────────────────────
    fy_col = next((c for c in df.columns if c.lower().startswith("fiscal_year") or c == "fy"), None)
    if fy_col is None and "period" in df.columns:
        fy_col = "period"
    if fy_col is not None:
        df_sorted = df.sort_values(fy_col)
    else:
        df_sorted = df

    lines = ["USCIS I-485 Processing Performance — Detailed Data:"]
    key_cols = [c for c in ["approval_rate", "throughput", "backlog_months",
                            "pending_change", "median_days", "cycle_time_days",
                            "receipts", "completions"] if c in df.columns]
    if key_cols:
        header_parts = [fy_col or "Row"] + key_cols
        lines.append("  " + " | ".join(header_parts))
        lines.append("  " + "-" * 60)
        for _, row in df_sorted.iterrows():
            fy_label = str(row[fy_col]) if fy_col else str(row.name)
            vals = [fy_label]
            for c in key_cols:
                v = row.get(c)
                if pd.notna(v):
                    if isinstance(v, float) and abs(v) < 10:
                        vals.append(f"{v:.2f}")
                    else:
                        vals.append(f"{v:,.0f}" if isinstance(v, (int, float)) else str(v))
                else:
                    vals.append("—")
            lines.append("  " + " | ".join(vals))
        chunks.append(_make_chunk("processing_times_trends.parquet",
                                  "processing_fy_table", "processing",
                                  "\n".join(lines)))

    # ── Trend analysis ───────────────────────────────────────────────────
    trend_lines = ["I-485 Processing Trend Analysis:"]
    if "backlog_months" in df.columns:
        bl = df["backlog_months"].dropna()
        if len(bl) > 0:
            trend_lines.append(f"  Backlog months: min={bl.min():.1f}, max={bl.max():.1f}, "
                               f"latest={bl.iloc[-1]:.1f}")
    if "approval_rate" in df.columns:
        ar = df["approval_rate"].dropna()
        if len(ar) > 0:
            trend_lines.append(f"  Approval rate: min={ar.min():.1%}, max={ar.max():.1%}, "
                               f"latest={ar.iloc[-1]:.1%}")
    if "throughput" in df.columns:
        tp = df["throughput"].dropna()
        if len(tp) > 0:
            trend_lines.append(f"  Throughput: min={tp.min():,.0f}, max={tp.max():,.0f}, "
                               f"latest={tp.iloc[-1]:,.0f}")
    if len(trend_lines) > 1:
        chunks.append(_make_chunk("processing_times_trends.parquet",
                                  "processing_trends", "processing",
                                  "\n".join(trend_lines)))


def _build_visa_bulletin_chunks(chunks: list) -> None:
    """Generate chunks for visa bulletin history."""
    path = ARTIFACTS_ROOT / "fact_cutoffs_all.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary_lines = [
        f"Visa Bulletin History (fact_cutoffs_all):",
        f"Total records: {len(df):,}",
        f"Columns: {', '.join(df.columns.tolist())}",
        f"Coverage: Visa bulletin cutoff dates from 2011 to 2026",
        f"Source: DOS Visa Bulletin PDFs (~180 bulletins parsed)",
        f"Use: Historical cutoff date tracking, retrogression analysis, movement patterns.",
    ]

    # Latest cutoff dates
    if "bulletin_date" in df.columns and "cutoff_date" in df.columns:
        latest = df.sort_values("bulletin_date", ascending=False).head(20)
        summary_lines.append("\nMost recent bulletin entries:")
        for _, row in latest.iterrows():
            parts = []
            for c in df.columns:
                if pd.notna(row.get(c)):
                    parts.append(f"{c}={row[c]}")
            summary_lines.append("  " + ", ".join(parts[:6]))

    chunks.append(_make_chunk("fact_cutoffs_all.parquet",
                              "visa_bulletin_summary", "visa_bulletin",
                              "\n".join(summary_lines)))

    # ── Per-category latest cutoff dates ──────────────────────────────────
    cat_col = next((c for c in df.columns if c in ("category", "preference_category", "visa_category")), None)
    country_col = next((c for c in df.columns if c in ("country", "chargeability_area")), None)
    bd_col = next((c for c in df.columns if c in ("bulletin_date", "bulletin_month")), None)
    cd_col = next((c for c in df.columns if c in ("cutoff_date",)), None)

    if all(c is not None for c in [cat_col, country_col, bd_col, cd_col]):
        # Latest cutoff per category × country
        idx = df.groupby([cat_col, country_col])[bd_col].idxmax()
        latest_cutoffs = df.loc[idx].sort_values([cat_col, country_col])

        lines = ["Latest Visa Bulletin Cutoff Dates (per EB category × country):"]
        current_cat = None
        for _, row in latest_cutoffs.iterrows():
            cat = row[cat_col]
            if cat != current_cat:
                lines.append(f"\n  {cat}:")
                current_cat = cat
            ctry = row[country_col]
            cutoff = str(row[cd_col])[:10] if pd.notna(row[cd_col]) else "Current"
            bulletin = str(row[bd_col])[:10] if pd.notna(row[bd_col]) else "?"
            lines.append(f"    {ctry}: cutoff={cutoff} (bulletin {bulletin})")

        chunks.append(_make_chunk("fact_cutoffs_all.parquet",
                                  "visa_bulletin_latest", "visa_bulletin",
                                  "\n".join(lines)))

    # ── Retrogression events ─────────────────────────────────────────────
    chart_col = next((c for c in df.columns if c in ("chart", "chart_type")), None)
    if all(c is not None for c in [cat_col, country_col, bd_col, cd_col]):
        df_sorted = df.sort_values([cat_col, country_col, bd_col])
        df_sorted["_prev_cutoff"] = df_sorted.groupby([cat_col, country_col])[cd_col].shift(1)
        retro = df_sorted[df_sorted[cd_col] < df_sorted["_prev_cutoff"]].copy()

        if len(retro) > 0:
            lines = [
                f"Retrogression Events Detected: {len(retro)} instances",
                f"Retrogression = cutoff date moves backward (bad for applicants).",
            ]
            retro_counts = retro.groupby([cat_col, country_col]).size().nlargest(15)
            lines.append("\nMost frequent retrogression by category × country:")
            for (cat, ctry), cnt in retro_counts.items():
                lines.append(f"  {cat} / {ctry}: {cnt} retrogressions")
            chunks.append(_make_chunk("fact_cutoffs_all.parquet",
                                      "visa_bulletin_retrogression", "visa_bulletin",
                                      "\n".join(lines)))
        df_sorted.drop(columns=["_prev_cutoff"], inplace=True, errors="ignore")


def _build_movement_chunks(chunks: list) -> None:
    """Generate chunks for category movement metrics."""
    path = ARTIFACTS_ROOT / "category_movement_metrics.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"Category Movement Metrics Summary:\n"
        f"Total records: {len(df):,}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Coverage: EB category × country movement analysis\n"
        f"Metrics: monthly advancement, volatility, retrogression frequency\n"
        f"Use: Compare EB2 vs EB3 movement, assess retrogression risk."
    )
    chunks.append(_make_chunk("category_movement_metrics.parquet",
                              "movement_summary", "visa_bulletin", summary,
                              _table_stats(df)))

    # ── Movement detail by category × country ────────────────────────────
    cat_col = next((c for c in df.columns if c in ("category", "preference_category")), None)
    country_col = next((c for c in df.columns if c in ("country", "chargeability_area")), None)
    if cat_col and country_col:
        movement_cols = [c for c in ["avg_monthly_advance_days", "median_advance_days",
                                     "volatility", "retrogression_count",
                                     "retrogression_pct", "longest_freeze_months"]
                         if c in df.columns]
        if movement_cols:
            latest = df.sort_values(movement_cols[0], ascending=False) if movement_cols else df
            lines = ["Category × Country Movement Analysis Detail:"]
            for _, row in latest.head(30).iterrows():
                cat = row.get(cat_col, "?")
                ctry = row.get(country_col, "?")
                parts = [f"{cat} / {ctry}:"]
                for mc in movement_cols:
                    v = row.get(mc)
                    if pd.notna(v):
                        if isinstance(v, float):
                            parts.append(f"{mc}={v:.1f}")
                        else:
                            parts.append(f"{mc}={v}")
                lines.append("  " + " ".join(parts))
            chunks.append(_make_chunk("category_movement_metrics.parquet",
                                      "movement_detail", "visa_bulletin",
                                      "\n".join(lines)))


def _build_backlog_chunks(chunks: list) -> None:
    """Generate chunks for backlog estimates."""
    path = ARTIFACTS_ROOT / "backlog_estimates.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"Backlog Estimates Summary:\n"
        f"Total records: {len(df):,}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Coverage: Estimated visa backlogs by category × country\n"
        f"Use: 'How many people are ahead of me?' — queue position estimation."
    )
    chunks.append(_make_chunk("backlog_estimates.parquet",
                              "backlog_summary", "pd_forecast", summary,
                              _table_stats(df)))

    # ── Backlog detail by category × country ─────────────────────────────
    cat_col = next((c for c in df.columns if c in ("category", "preference_category")), None)
    country_col = next((c for c in df.columns if c in ("country", "chargeability_area")), None)
    bl_col = next((c for c in df.columns if "backlog" in c.lower() or "pending" in c.lower()
                   or "queue" in c.lower()), None)
    if cat_col and country_col and bl_col:
        latest_year = None
        yr_col = next((c for c in df.columns if c in ("fiscal_year", "year", "bulletin_year")), None)
        if yr_col:
            latest_year = df[yr_col].max()
            detail = df[df[yr_col] == latest_year].copy()
        else:
            detail = df.copy()

        detail_sorted = detail.sort_values(bl_col, ascending=False)
        lines = [f"Backlog Estimates Detail{' (latest year: ' + str(latest_year) + ')' if latest_year else ''}:"]
        for _, row in detail_sorted.head(30).iterrows():
            cat = row.get(cat_col, "?")
            ctry = row.get(country_col, "?")
            bl = row.get(bl_col, 0)
            bl_str = f"{bl:,.0f}" if isinstance(bl, (int, float)) and pd.notna(bl) else str(bl)
            lines.append(f"  {cat} / {ctry}: {bl_col}={bl_str}")
        chunks.append(_make_chunk("backlog_estimates.parquet",
                                  "backlog_detail", "pd_forecast",
                                  "\n".join(lines)))


def _build_visa_demand_chunks(chunks: list) -> None:
    """Generate chunks for visa demand metrics."""
    path = ARTIFACTS_ROOT / "visa_demand_metrics.parquet"
    df = _safe_read(path)
    if df is None:
        return

    summary = (
        f"Visa Demand Metrics Summary:\n"
        f"Total records: {len(df):,}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Sources: DOS immigrant visa issuances, visa applications, NIV statistics\n"
        f"Use: Track visa demand by category × country, identify trends."
    )
    chunks.append(_make_chunk("visa_demand_metrics.parquet",
                              "visa_demand_summary", "visa_demand", summary,
                              _table_stats(df)))

    # ── Top countries by demand ──────────────────────────────────────────
    country_col = next((c for c in df.columns if c in ("country", "nationality",
                                                        "chargeability_area",
                                                        "country_of_birth")), None)
    count_col = next((c for c in df.columns if c in ("count_issued", "count", "total",
                                                      "applications", "issuances")), None)
    if country_col and count_col:
        by_country = df.groupby(country_col)[count_col].sum().nlargest(25)
        lines = ["Top 25 Countries by Visa Demand:"]
        for ctry, cnt in by_country.items():
            lines.append(f"  {ctry}: {cnt:,.0f}")
        chunks.append(_make_chunk("visa_demand_metrics.parquet",
                                  "visa_demand_top_countries", "visa_demand",
                                  "\n".join(lines)))

    # ── Year-over-year trends ────────────────────────────────────────────
    fy_col = next((c for c in df.columns if c in ("fiscal_year", "year", "fy")), None)
    cat_col = next((c for c in df.columns if c in ("category", "visa_category",
                                                    "visa_class")), None)
    if fy_col and count_col:
        by_year = df.groupby(fy_col)[count_col].sum().sort_index()
        lines = ["Visa Demand Year-over-Year Trends:"]
        for yr, cnt in by_year.items():
            lines.append(f"  {yr}: {cnt:,.0f}")
        if cat_col:
            lines.append("\nDemand by Category × Year (latest 3 years):")
            recent_years = sorted(df[fy_col].unique())[-3:]
            recent = df[df[fy_col].isin(recent_years)]
            pivot = recent.groupby([cat_col, fy_col])[count_col].sum().unstack(fill_value=0)
            for cat in pivot.index[:15]:  # cap to 15 categories
                vals = " | ".join(f"{pivot.loc[cat, y]:,.0f}" for y in pivot.columns)
                lines.append(f"  {cat}: {vals}")
        chunks.append(_make_chunk("visa_demand_metrics.parquet",
                                  "visa_demand_trends", "visa_demand",
                                  "\n".join(lines)))


def _build_iv_post_chunks(chunks: list) -> None:
    """Generate chunks for IV issuances by consular post.
    
    Source: fact_iv_post.parquet — monthly immigrant visa issuances
    by consular post (city) and visa class. Enables queries like
    'How many F1 visas were issued in Amsterdam in Feb 2025?'
    """
    path = ARTIFACTS_ROOT / "fact_iv_post.parquet"
    df = _safe_read(path)
    if df is None:
        return

    # Summary chunk
    posts = df["post"].nunique() if "post" in df.columns else 0
    classes = df["visa_class"].nunique() if "visa_class" in df.columns else 0
    fys = sorted(df["fiscal_year"].unique().tolist()) if "fiscal_year" in df.columns else []
    months = sorted(df["month"].unique().tolist()) if "month" in df.columns else []

    summary = (
        f"Immigrant Visa Issuances by Consular Post:\n"
        f"Total records: {len(df):,}\n"
        f"Consular posts: {posts} (e.g., Amsterdam, Chennai, London, Mexico City)\n"
        f"Visa classes: {classes} (F1, E2, IR1, DV, etc.)\n"
        f"Fiscal years: {', '.join(str(y) for y in fys)}\n"
        f"Months available: {', '.join(months)}\n"
        f"Granularity: monthly × post × visa_class\n"
        f"Source: DOS 'IV Issuances by Post and Visa Class' monthly PDFs\n"
        f"Use: Answer questions about visa issuance volume at specific consular posts."
    )
    chunks.append(_make_chunk("fact_iv_post.parquet",
                              "iv_post_summary", "visa_demand", summary,
                              _table_stats(df)))

    # Top posts by total issuances
    if "post" in df.columns and "issued" in df.columns:
        top_posts = df.groupby("post")["issued"].sum().nlargest(30)
        lines = ["Top 30 Consular Posts by Total IV Issuances:"]
        for post, cnt in top_posts.items():
            lines.append(f"  {post}: {cnt:,.0f}")
        chunks.append(_make_chunk("fact_iv_post.parquet",
                                  "iv_post_top_posts", "visa_demand",
                                  "\n".join(lines)))

    # Top visa classes
    if "visa_class" in df.columns and "issued" in df.columns:
        top_classes = df.groupby("visa_class")["issued"].sum().nlargest(20)
        lines = ["Top 20 Visa Classes by IV Issuances (all posts):"]
        for vc, cnt in top_classes.items():
            lines.append(f"  {vc}: {cnt:,.0f}")
        chunks.append(_make_chunk("fact_iv_post.parquet",
                                  "iv_post_top_classes", "visa_demand",
                                  "\n".join(lines)))

    # Recent month detail (latest available month)
    if "calendar_year" in df.columns and "month" in df.columns:
        latest = df.sort_values(["calendar_year", "month"]).iloc[-1]
        latest_month = latest["month"]
        latest_year = latest["calendar_year"]
        recent = df[(df["month"] == latest_month) & (df["calendar_year"] == latest_year)]
        top_recent = recent.groupby("post")["issued"].sum().nlargest(15)
        lines = [f"IV Issuances for {latest_month} {latest_year} — Top 15 Posts:"]
        for post, cnt in top_recent.items():
            lines.append(f"  {post}: {cnt:,.0f}")
        chunks.append(_make_chunk("fact_iv_post.parquet",
                                  "iv_post_recent_month", "visa_demand",
                                  "\n".join(lines)))


def _build_lca_perm_chunks(chunks: list) -> None:
    """Generate chunks for LCA and PERM filing data — the backbone DOL tables."""
    # ── LCA summary & stats ──────────────────────────────────────────────
    lca_path = ARTIFACTS_ROOT / "fact_lca"
    lca = _safe_read(lca_path)
    if lca is not None and len(lca) > 0:
        fys = sorted(lca["fiscal_year"].dropna().unique().tolist()) if "fiscal_year" in lca.columns else []
        statuses = lca["case_status"].value_counts().to_dict() if "case_status" in lca.columns else {}
        visa_classes = lca["visa_class"].value_counts().head(10).to_dict() if "visa_class" in lca.columns else {}

        summary = (
            f"LCA (Labor Condition Application) Filing Data:\n"
            f"Total records: {len(lca):,}\n"
            f"Fiscal years: {', '.join(str(y) for y in fys)}\n"
            f"Case status distribution: {statuses}\n"
            f"Top visa classes: {visa_classes}\n"
            f"Key columns: employer, SOC code, job title, wage, worksite\n"
            f"Source: DOL disclosure data (H-1B, H-1B1, E-3 filings)\n"
            f"Use: Track H-1B sponsorship volume, employer filing patterns, wage offers."
        )
        chunks.append(_make_chunk("fact_lca", "lca_summary", "filings", summary))

        # Top LCA employers
        if "employer_name_raw" in lca.columns:
            emp_col = "employer_name_raw"
        elif "employer_name" in lca.columns:
            emp_col = "employer_name"
        else:
            emp_col = None

        if emp_col:
            top_emp = lca[emp_col].value_counts().head(40)
            lines = ["Top 40 LCA Employers by Filing Volume (all years):"]
            for emp, cnt in top_emp.items():
                lines.append(f"  {emp}: {cnt:,}")
            chunks.append(_make_chunk("fact_lca", "lca_top_employers",
                                      "filings", "\n".join(lines)))

        # LCA fiscal year trends
        if "fiscal_year" in lca.columns:
            by_fy = lca.groupby("fiscal_year").size().sort_index()
            lines = ["LCA Filings by Fiscal Year:"]
            for fy, cnt in by_fy.items():
                lines.append(f"  FY{fy}: {cnt:,}")
            chunks.append(_make_chunk("fact_lca", "lca_fy_trends",
                                      "filings", "\n".join(lines)))

        # LCA wage distribution
        wage_col = next((c for c in lca.columns if c in ("wage_rate_from", "wage_offer_from")), None)
        if wage_col:
            wages = lca[wage_col].dropna()
            annual = wages[wages > 10000]  # filter out hourlies
            if len(annual) > 0:
                lines = [
                    "LCA Wage Distribution (annual rates only, >$10K):",
                    f"  Records: {len(annual):,}",
                    f"  Min: ${annual.min():,.0f}",
                    f"  25th pct: ${annual.quantile(0.25):,.0f}",
                    f"  Median: ${annual.median():,.0f}",
                    f"  75th pct: ${annual.quantile(0.75):,.0f}",
                    f"  Max: ${annual.max():,.0f}",
                ]
                chunks.append(_make_chunk("fact_lca", "lca_wage_distribution",
                                          "filings", "\n".join(lines)))

    # ── PERM summary & stats ─────────────────────────────────────────────
    perm_path = ARTIFACTS_ROOT / "fact_perm"
    perm = _safe_read(perm_path)
    if perm is not None and len(perm) > 0:
        fys = sorted(perm["fiscal_year"].dropna().unique().tolist()) if "fiscal_year" in perm.columns else []
        statuses = perm["case_status"].value_counts().to_dict() if "case_status" in perm.columns else {}

        summary = (
            f"PERM (Labor Certification) Filing Data:\n"
            f"Total records: {len(perm):,}\n"
            f"Fiscal years: {', '.join(str(y) for y in fys)}\n"
            f"Case status distribution: {statuses}\n"
            f"Key columns: case_number, employer, SOC, job_title, wages, worksite\n"
            f"Source: DOL PERM disclosure data\n"
            f"Use: Track EB-2/EB-3 green card sponsorship, approval/denial rates, employer patterns.\n"
            f"Note: PERM is the first step in most employment-based green card applications."
        )
        chunks.append(_make_chunk("fact_perm", "perm_summary", "filings", summary))

        # Top PERM employers
        if "employer_name" in perm.columns:
            top_emp = perm["employer_name"].value_counts().head(40)
            lines = ["Top 40 PERM Employers by Filing Volume (all years):"]
            for emp, cnt in top_emp.items():
                lines.append(f"  {emp}: {cnt:,}")
            chunks.append(_make_chunk("fact_perm", "perm_top_employers",
                                      "filings", "\n".join(lines)))

        # PERM approval/denial rates by FY
        if "fiscal_year" in perm.columns and "case_status" in perm.columns:
            lines = ["PERM Approval/Denial Rates by Fiscal Year:"]
            for fy in sorted(perm["fiscal_year"].dropna().unique()):
                fy_data = perm[perm["fiscal_year"] == fy]
                total = len(fy_data)
                certified = (fy_data["case_status"].str.upper() == "CERTIFIED").sum()
                denied = (fy_data["case_status"].str.upper() == "DENIED").sum()
                rate = certified / total * 100 if total > 0 else 0
                lines.append(f"  FY{fy}: {total:,} total, {certified:,} certified "
                             f"({rate:.1f}%), {denied:,} denied")
            chunks.append(_make_chunk("fact_perm", "perm_approval_rates",
                                      "filings", "\n".join(lines)))


def _build_employer_extended_chunks(chunks: list) -> None:
    """Generate chunks for employer dim, features, monthly metrics, risk, and ML scores."""
    # ── dim_employer summary ─────────────────────────────────────────────
    de_path = ARTIFACTS_ROOT / "dim_employer.parquet"
    de = _safe_read(de_path)
    if de is not None and len(de) > 0:
        lines = [
            f"Employer Directory (dim_employer):",
            f"Total employers: {len(de):,}",
            f"Columns: {', '.join(de.columns.tolist())}",
            f"Source: De-duplicated from PERM, LCA, and H-1B filings",
            f"Use: Canonical employer reference — employer_id links to all other employer tables.",
        ]
        if "aliases" in de.columns:
            has_aliases = de["aliases"].dropna()
            has_aliases = has_aliases[has_aliases.str.len() > 2]
            lines.append(f"  Employers with aliases: {len(has_aliases):,}")
        chunks.append(_make_chunk("dim_employer.parquet", "dim_employer_summary",
                                  "employer", "\n".join(lines)))

    # ── employer_features summary ────────────────────────────────────────
    ef_path = ARTIFACTS_ROOT / "employer_features.parquet"
    ef = _safe_read(ef_path)
    if ef is not None and len(ef) > 0:
        lines = [
            f"Employer Features Table:",
            f"Total employers with features: {len(ef):,}",
            f"Feature columns: {', '.join(ef.columns.tolist())}",
            f"Windows: 12-month, 24-month, 36-month rolling from latest PERM data",
        ]
        for col in ["approval_rate_24m", "wage_ratio_med", "n_24m"]:
            if col in ef.columns:
                vals = ef[col].dropna()
                if len(vals) > 0:
                    lines.append(f"  {col}: mean={vals.mean():.2f}, median={vals.median():.2f}")
        chunks.append(_make_chunk("employer_features.parquet",
                                  "employer_features_summary", "employer",
                                  "\n".join(lines)))

    # ── employer_monthly_metrics summary ─────────────────────────────────
    emm_path = ARTIFACTS_ROOT / "employer_monthly_metrics.parquet"
    emm = _safe_read(emm_path)
    if emm is not None and len(emm) > 0:
        n_employers = emm["employer_id"].nunique() if "employer_id" in emm.columns else 0
        months_range = ""
        if "month" in emm.columns:
            months = pd.to_datetime(emm["month"], errors="coerce").dropna()
            if len(months) > 0:
                months_range = f"{months.min().strftime('%Y-%m')} to {months.max().strftime('%Y-%m')}"

        lines = [
            f"Employer Monthly Metrics:",
            f"Total records: {len(emm):,} ({n_employers:,} unique employers)",
            f"Date range: {months_range}",
            f"Columns: {', '.join(emm.columns.tolist())}",
            f"Use: Track monthly filing volume, approval/denial/audit rates per employer.",
        ]
        # Top employers by total filings
        if "employer_name" in emm.columns and "filings" in emm.columns:
            by_emp = emm.groupby("employer_name")["filings"].sum().nlargest(20)
            lines.append("\nTop 20 employers by total monthly filings:")
            for emp, cnt in by_emp.items():
                lines.append(f"  {emp}: {cnt:,.0f}")
        chunks.append(_make_chunk("employer_monthly_metrics.parquet",
                                  "employer_monthly_summary", "employer",
                                  "\n".join(lines)))

    # ── employer_risk_features ───────────────────────────────────────────
    erf_path = ARTIFACTS_ROOT / "employer_risk_features.parquet"
    erf = _safe_read(erf_path)
    if erf is not None and len(erf) > 0:
        lines = [
            f"Employer Risk Features (WARN Act linkage):",
            f"Total employers flagged: {len(erf):,}",
            f"Columns: {', '.join(erf.columns.tolist())}",
            f"These employers had WARN Act layoff events that may affect sponsorship.",
        ]
        if "employer_name_raw" in erf.columns and "total_employees_affected" in erf.columns:
            top = erf.nlargest(20, "total_employees_affected")
            lines.append("\nTop 20 employers by employees affected in layoffs:")
            for _, row in top.iterrows():
                name = row["employer_name_raw"]
                affected = row["total_employees_affected"]
                events = row.get("total_warn_events", "?")
                lines.append(f"  {name}: {affected:,.0f} employees, {events} events")
        chunks.append(_make_chunk("employer_risk_features.parquet",
                                  "employer_risk_summary", "employer",
                                  "\n".join(lines)))

    # ── employer_friendliness_scores_ml ──────────────────────────────────
    ml_path = ARTIFACTS_ROOT / "employer_friendliness_scores_ml.parquet"
    ml = _safe_read(ml_path)
    if ml is not None and len(ml) > 0:
        lines = [
            f"ML-Based Employer Friendliness Scores:",
            f"Total employers scored: {len(ml):,}",
            f"Columns: {', '.join(ml.columns.tolist())}",
            f"Model: XGBoost classifier calibrated on PERM case outcomes",
            f"Score (efs_ml): 0–100 scale based on calibrated approval probability",
        ]
        if "efs_ml" in ml.columns:
            vals = ml["efs_ml"].dropna()
            lines.append(f"  Score stats: mean={vals.mean():.1f}, median={vals.median():.1f}, "
                         f"std={vals.std():.1f}")
        chunks.append(_make_chunk("employer_friendliness_scores_ml.parquet",
                                  "efs_ml_summary", "employer",
                                  "\n".join(lines)))


def _build_niv_issuance_chunks(chunks: list) -> None:
    """Generate chunks for nonimmigrant visa (NIV) issuance data."""
    path = ARTIFACTS_ROOT / "fact_niv_issuance.parquet"
    df = _safe_read(path)
    if df is None or len(df) == 0:
        return

    fys = sorted(df["fiscal_year"].unique().tolist()) if "fiscal_year" in df.columns else []
    n_countries = df["country"].nunique() if "country" in df.columns else 0
    n_classes = df["visa_class"].nunique() if "visa_class" in df.columns else 0

    summary = (
        f"Nonimmigrant Visa (NIV) Issuance Data:\n"
        f"Total records: {len(df):,}\n"
        f"Fiscal years: {', '.join(str(y) for y in fys)}\n"
        f"Countries: {n_countries}\n"
        f"Visa classes: {n_classes} (H-1B, L-1, F-1, B-1/B-2, J-1, etc.)\n"
        f"Source: DOS NIV Statistics PDFs and spreadsheets\n"
        f"Use: Track nonimmigrant visa demand by country and class, year-over-year trends."
    )
    chunks.append(_make_chunk("fact_niv_issuance.parquet",
                              "niv_issuance_summary", "visa_demand", summary))

    # Top countries
    if "country" in df.columns and "issued" in df.columns:
        top = df.groupby("country")["issued"].sum().nlargest(25)
        lines = ["Top 25 Countries by NIV Issuances (all years):"]
        for ctry, cnt in top.items():
            lines.append(f"  {ctry}: {cnt:,}")
        chunks.append(_make_chunk("fact_niv_issuance.parquet",
                                  "niv_top_countries", "visa_demand",
                                  "\n".join(lines)))

    # Top visa classes
    if "visa_class" in df.columns and "issued" in df.columns:
        top = df.groupby("visa_class")["issued"].sum().nlargest(20)
        lines = ["Top 20 NIV Visa Classes by Issuances:"]
        for vc, cnt in top.items():
            lines.append(f"  {vc}: {cnt:,}")
        chunks.append(_make_chunk("fact_niv_issuance.parquet",
                                  "niv_top_classes", "visa_demand",
                                  "\n".join(lines)))

    # Year-over-year totals
    if "fiscal_year" in df.columns and "issued" in df.columns:
        by_fy = df.groupby("fiscal_year")["issued"].sum().sort_index()
        lines = ["NIV Issuances by Fiscal Year:"]
        for fy, cnt in by_fy.items():
            lines.append(f"  FY{fy}: {cnt:,}")
        chunks.append(_make_chunk("fact_niv_issuance.parquet",
                                  "niv_fy_trends", "visa_demand",
                                  "\n".join(lines)))


def _build_iv_issuance_detail_chunks(chunks: list) -> None:
    """Generate chunks for immigrant visa issuance and applications."""
    # ── fact_visa_issuance (IV by country/category) ──────────────────────
    vi_path = ARTIFACTS_ROOT / "fact_visa_issuance.parquet"
    vi = _safe_read(vi_path)
    if vi is not None and len(vi) > 0:
        lines = [
            f"Immigrant Visa Issuances by Country (fact_visa_issuance):",
            f"Total records: {len(vi):,}",
            f"Fiscal years: {sorted(vi['fiscal_year'].unique().tolist()) if 'fiscal_year' in vi.columns else 'N/A'}",
            f"Source: DOS Visa Annual Reports",
            f"Use: Track immigrant visa issuances by chargeability country and category.",
        ]
        if "country" in vi.columns and "issued" in vi.columns:
            top = vi.groupby("country")["issued"].sum().nlargest(20)
            lines.append("\nTop 20 countries by IV issuances:")
            for ctry, cnt in top.items():
                lines.append(f"  {ctry}: {cnt:,}")
        chunks.append(_make_chunk("fact_visa_issuance.parquet",
                                  "iv_issuance_by_country", "visa_demand",
                                  "\n".join(lines)))

    # ── fact_visa_applications (by FSC) ──────────────────────────────────
    va_path = ARTIFACTS_ROOT / "fact_visa_applications.parquet"
    va = _safe_read(va_path)
    if va is not None and len(va) > 0:
        lines = [
            f"Visa Applications by Foreign State of Chargeability (fact_visa_applications):",
            f"Total records: {len(va):,}",
            f"Fiscal years: {sorted(va['fiscal_year'].unique().tolist()) if 'fiscal_year' in va.columns else 'N/A'}",
            f"Source: DOS IV Issuances by FSC PDFs",
            f"Use: Track visa applications, refusals, and issuances by country and class.",
        ]
        if "country" in va.columns and "applications" in va.columns:
            top = va.groupby("country")["applications"].sum().nlargest(20)
            lines.append("\nTop 20 countries by visa applications:")
            for ctry, cnt in top.items():
                lines.append(f"  {ctry}: {cnt:,}")
        if "refusals" in va.columns:
            total_apps = va["applications"].sum()
            total_ref = va["refusals"].sum()
            if total_apps > 0:
                lines.append(f"\nOverall refusal rate: {total_ref/total_apps*100:.1f}% "
                             f"({total_ref:,.0f} of {total_apps:,.0f})")
        chunks.append(_make_chunk("fact_visa_applications.parquet",
                                  "visa_applications_summary", "visa_demand",
                                  "\n".join(lines)))


def _build_oews_detail_chunks(chunks: list) -> None:
    """Generate chunks for OEWS wage detail (fact_oews)."""
    path = ARTIFACTS_ROOT / "fact_oews"
    df = _safe_read(path)
    if df is None or len(df) == 0:
        return

    n_socs = df["soc_code"].nunique() if "soc_code" in df.columns else 0
    n_areas = df["area_code"].nunique() if "area_code" in df.columns else 0
    ref_years = sorted(df["ref_year"].unique().tolist()) if "ref_year" in df.columns else []

    summary = (
        f"OEWS Wage Data (fact_oews):\n"
        f"Total records: {len(df):,} (SOC × area combinations)\n"
        f"SOC codes: {n_socs}\n"
        f"Geographic areas: {n_areas}\n"
        f"Reference years: {', '.join(str(y) for y in ref_years)}\n"
        f"Key wage columns: a_mean, a_median, a_pct10–a_pct90 (annual), h_* (hourly)\n"
        f"Source: BLS Occupational Employment and Wage Statistics\n"
        f"Use: Prevailing wage lookups, wage benchmarking for immigration cases."
    )
    chunks.append(_make_chunk("fact_oews", "oews_data_summary", "salary", summary))

    # Top-paying occupations (national level)
    if "a_median" in df.columns and "soc_code" in df.columns:
        nat = df.copy()
        if "area_code" in df.columns:
            nat_mask = df["area_code"].astype(str).isin(["1", "0", "C0000", "000000"])
            if nat_mask.sum() > 50:
                nat = df[nat_mask]
        top = nat.dropna(subset=["a_median"]).nlargest(25, "a_median")
        lines = ["Top 25 Highest-Paying SOC Codes (OEWS national median):"]
        for _, row in top.iterrows():
            code = row["soc_code"]
            med = row["a_median"]
            lines.append(f"  {code}: ${med:,.0f}/year")
        chunks.append(_make_chunk("fact_oews", "oews_top_paying",
                                  "salary", "\n".join(lines)))


def _build_dim_extended_chunks(chunks: list) -> None:
    """Generate chunks for dim_soc and dim_area dimension tables."""
    # ── dim_soc ──────────────────────────────────────────────────────────
    soc_path = ARTIFACTS_ROOT / "dim_soc.parquet"
    soc = _safe_read(soc_path)
    if soc is not None and len(soc) > 0:
        lines = [
            f"SOC Code Directory (dim_soc):",
            f"Total SOC codes: {len(soc):,}",
            f"Columns: {', '.join(soc.columns.tolist())}",
        ]
        if "soc_version" in soc.columns:
            versions = soc["soc_version"].value_counts().to_dict()
            lines.append(f"Versions: {versions}")
        if "soc_major_group" in soc.columns:
            groups = soc["soc_major_group"].value_counts().head(10)
            lines.append("\nTop 10 SOC major groups:")
            for grp, cnt in groups.items():
                lines.append(f"  {grp}: {cnt} codes")
        if "soc_code" in soc.columns and "soc_title" in soc.columns:
            # Show immigration-heavy codes
            tech_codes = ["15-1252", "15-1256", "15-1211", "15-1299", "17-2061",
                          "15-2051", "11-3021", "29-1141", "13-1111", "13-2011"]
            lines.append("\nCommon immigration-sponsored SOC codes:")
            for code in tech_codes:
                match = soc[soc["soc_code"] == code]
                if len(match) > 0:
                    title = match.iloc[0]["soc_title"]
                    lines.append(f"  {code}: {title}")
        chunks.append(_make_chunk("dim_soc.parquet", "dim_soc_summary",
                                  "occupation", "\n".join(lines)))

    # ── dim_area ─────────────────────────────────────────────────────────
    area_path = ARTIFACTS_ROOT / "dim_area.parquet"
    area = _safe_read(area_path)
    if area is not None and len(area) > 0:
        lines = [
            f"Geographic Area Directory (dim_area):",
            f"Total areas: {len(area):,}",
            f"Columns: {', '.join(area.columns.tolist())}",
        ]
        if "area_type" in area.columns:
            types = area["area_type"].value_counts().to_dict()
            lines.append(f"Area types: {types}")
        if "state_abbr" in area.columns:
            states = area["state_abbr"].dropna().nunique()
            lines.append(f"States represented: {states}")
        if "area_title" in area.columns:
            lines.append(f"\nSample metro areas: {', '.join(area['area_title'].head(10).tolist())}")
        chunks.append(_make_chunk("dim_area.parquet", "dim_area_summary",
                                  "geographic", "\n".join(lines)))


def _build_queue_depth_chunks(chunks: list) -> None:
    """Generate chunks for queue depth and wait time estimates."""
    path = ARTIFACTS_ROOT / "queue_depth_estimates.parquet"
    df = _safe_read(path)
    if df is None or len(df) == 0:
        return

    summary = (
        f"Queue Depth & Wait Time Estimates:\n"
        f"Total records: {len(df):,}\n"
        f"Columns: {', '.join(df.columns.tolist())}\n"
        f"Coverage: EB category × country × priority date month\n"
        f"Key metrics: est_wait_years, est_months_to_current, confidence\n"
        f"Use: 'How long will I wait?' — estimated queue position and wait time."
    )
    chunks.append(_make_chunk("queue_depth_estimates.parquet",
                              "queue_depth_summary", "pd_forecast", summary,
                              _table_stats(df)))

    # Highest wait times
    if "est_wait_years" in df.columns and "category" in df.columns and "country" in df.columns:
        top_waits = df.nlargest(20, "est_wait_years")
        lines = ["Longest Estimated Wait Times (top 20):"]
        for _, row in top_waits.iterrows():
            cat = row.get("category", "?")
            ctry = row.get("country", "?")
            wait = row["est_wait_years"]
            conf = row.get("confidence", "?")
            lines.append(f"  {cat} / {ctry}: {wait:.1f} years (confidence: {conf})")
        chunks.append(_make_chunk("queue_depth_estimates.parquet",
                                  "queue_longest_waits", "pd_forecast",
                                  "\n".join(lines)))


def _build_h1b_hub_chunks(chunks: list) -> None:
    """Generate chunks for H-1B Employer Hub data (historical, stale after FY2023)."""
    path = ARTIFACTS_ROOT / "fact_h1b_employer_hub.parquet"
    df = _safe_read(path)
    if df is None or len(df) == 0:
        return

    fys = sorted(df["fiscal_year"].unique().tolist()) if "fiscal_year" in df.columns else []
    n_employers = df["employer_name"].nunique() if "employer_name" in df.columns else 0

    summary = (
        f"H-1B Employer Data Hub (fact_h1b_employer_hub):\n"
        f"Total records: {len(df):,} ({n_employers:,} unique employers)\n"
        f"Fiscal years: {', '.join(str(y) for y in fys)}\n"
        f"⚠️ STALE DATA — USCIS discontinued after FY2023. All rows marked is_stale=True.\n"
        f"Key columns: employer, initial/continuing approvals+denials, NAICS, state, city\n"
        f"Source: USCIS H-1B Employer Data Hub CSV files\n"
        f"Use: Historical H-1B petition trends by employer. Good for long-term patterns."
    )
    chunks.append(_make_chunk("fact_h1b_employer_hub.parquet",
                              "h1b_hub_summary", "employer", summary))

    # Top H-1B employers
    if "employer_name" in df.columns and "total_petitions" in df.columns:
        top = df.groupby("employer_name")["total_petitions"].sum().nlargest(30)
        lines = ["Top 30 H-1B Employers by Total Petitions (historical FY2009–FY2023):"]
        for emp, cnt in top.items():
            lines.append(f"  {emp}: {cnt:,}")
        chunks.append(_make_chunk("fact_h1b_employer_hub.parquet",
                                  "h1b_hub_top_employers", "employer",
                                  "\n".join(lines)))

    # H-1B filings by FY
    if "fiscal_year" in df.columns and "total_petitions" in df.columns:
        by_fy = df.groupby("fiscal_year")["total_petitions"].sum().sort_index()
        lines = ["H-1B Petitions by Fiscal Year (Employer Hub data):"]
        for fy, cnt in by_fy.items():
            lines.append(f"  FY{fy}: {cnt:,}")
        if "approval_rate" in df.columns:
            fy_rates = df.groupby("fiscal_year")["approval_rate"].mean()
            lines.append("\nAverage approval rate by FY:")
            for fy, rate in fy_rates.items():
                lines.append(f"  FY{fy}: {rate:.1%}")
        chunks.append(_make_chunk("fact_h1b_employer_hub.parquet",
                                  "h1b_hub_fy_trends", "employer",
                                  "\n".join(lines)))


def _build_small_table_chunks(chunks: list) -> None:
    """Generate chunks for smaller reference tables: BLS CES, USCIS approvals,
    waiting list, and cutoff trends."""
    # ── fact_bls_ces (26 rows) ───────────────────────────────────────────
    bls_path = ARTIFACTS_ROOT / "fact_bls_ces.parquet"
    bls = _safe_read(bls_path)
    if bls is not None and len(bls) > 0:
        lines = [
            f"BLS Current Employment Statistics (fact_bls_ces):",
            f"Total records: {len(bls):,}",
            f"Columns: {', '.join(bls.columns.tolist())}",
            f"Source: BLS CES API (employment levels, nonfarm payroll)",
            f"Use: Macro employment trends for immigration context.",
        ]
        if "series_title" in bls.columns:
            for _, row in bls.iterrows():
                title = row.get("series_title", "")
                value = row.get("value", "")
                year = row.get("year", "")
                period = row.get("period_name", "")
                lines.append(f"  {title}: {value} ({period} {year})")
        chunks.append(_make_chunk("fact_bls_ces.parquet",
                                  "bls_ces_summary", "salary",
                                  "\n".join(lines)))

    # ── fact_uscis_approvals (146 rows) ──────────────────────────────────
    ua_path = ARTIFACTS_ROOT / "fact_uscis_approvals.parquet"
    ua = _safe_read(ua_path)
    if ua is not None and len(ua) > 0:
        lines = [
            f"USCIS Approval/Denial Statistics (fact_uscis_approvals):",
            f"Total records: {len(ua):,}",
            f"Columns: {', '.join(ua.columns.tolist())}",
        ]
        if "fiscal_year" in ua.columns:
            fys = sorted(ua["fiscal_year"].unique().tolist())
            lines.append(f"Fiscal years: {', '.join(str(y) for y in fys)}")
        if "form" in ua.columns:
            forms = ua["form"].unique().tolist()
            lines.append(f"Forms covered: {', '.join(str(f) for f in forms)}")
        if "approvals" in ua.columns and "denials" in ua.columns:
            total_approvals = ua["approvals"].sum()
            total_denials = ua["denials"].sum()
            lines.append(f"Total approvals: {total_approvals:,.0f}, denials: {total_denials:,.0f}")
        # Show detail
        if "category" in ua.columns and "approvals" in ua.columns:
            lines.append("\nApprovals by category:")
            by_cat = ua.groupby("category")["approvals"].sum().nlargest(15)
            for cat, cnt in by_cat.items():
                lines.append(f"  {cat}: {cnt:,.0f}")
        chunks.append(_make_chunk("fact_uscis_approvals.parquet",
                                  "uscis_approvals_summary", "processing",
                                  "\n".join(lines)))

    # ── fact_waiting_list (9 rows) ───────────────────────────────────────
    wl_path = ARTIFACTS_ROOT / "fact_waiting_list.parquet"
    wl = _safe_read(wl_path)
    if wl is not None and len(wl) > 0:
        lines = [
            f"DOS Visa Waiting List (fact_waiting_list):",
            f"Total records: {len(wl):,}",
            f"Source: DOS Annual Report of Immigrant Visa Applicants",
            f"Use: Track how many people are registered in the visa waiting list by category.",
        ]
        for _, row in wl.iterrows():
            cat = row.get("category", "?")
            ctry = row.get("country", "?")
            count = row.get("count_waiting", 0)
            year = row.get("report_year", "?")
            lines.append(f"  {cat} / {ctry}: {count:,.0f} (report year {year})")
        chunks.append(_make_chunk("fact_waiting_list.parquet",
                                  "waiting_list_summary", "visa_bulletin",
                                  "\n".join(lines)))

    # ── fact_cutoff_trends (8,315 rows) ──────────────────────────────────
    ct_path = ARTIFACTS_ROOT / "fact_cutoff_trends.parquet"
    ct = _safe_read(ct_path)
    if ct is not None and len(ct) > 0:
        lines = [
            f"Visa Bulletin Cutoff Trends (fact_cutoff_trends):",
            f"Total records: {len(ct):,}",
            f"Columns: {', '.join(ct.columns.tolist())}",
            f"Key metrics: monthly_advancement_days, velocity_3m/6m, retrogression_flag",
            f"Use: Analyze cutoff date movement speed and retrogression patterns.",
        ]
        if "retrogression_flag" in ct.columns:
            n_retro = ct["retrogression_flag"].sum()
            lines.append(f"Total retrogression events: {n_retro:,} of {len(ct):,} months "
                         f"({n_retro/len(ct)*100:.1f}%)")
        if "monthly_advancement_days" in ct.columns:
            adv = ct["monthly_advancement_days"].dropna()
            lines.append(f"Advancement days: mean={adv.mean():.1f}, median={adv.median():.1f}")
        chunks.append(_make_chunk("fact_cutoff_trends.parquet",
                                  "cutoff_trends_summary", "visa_bulletin",
                                  "\n".join(lines)))


def _build_dimension_chunks(chunks: list) -> None:
    """Generate chunks for dimension/lookup tables."""
    dims = {
        "dim_country.parquet": {
            "topic": "general",
            "desc": "ISO 3166-1 country lookup (249 countries). Columns: country_code, country_name, region, etc.",
        },
        "dim_visa_class.parquet": {
            "topic": "visa_bulletin",
            "desc": "EB visa category lookup (EB1–EB5 + Other). Includes preference descriptions.",
        },
        "dim_visa_ceiling.parquet": {
            "topic": "visa_bulletin",
            "desc": "Annual visa allocation limits by category. Used for backlog context.",
        },
    }
    for fname, info in dims.items():
        path = ARTIFACTS_ROOT / fname
        df = _safe_read(path)
        if df is None:
            continue
        text = (
            f"{fname} — {info['desc']}\n"
            f"Rows: {len(df)}, Columns: {', '.join(df.columns.tolist())}"
        )
        chunks.append(_make_chunk(fname, f"dim_{fname.replace('.parquet', '')}",
                                  info["topic"], text))

    # ── WARN layoff events ───────────────────────────────────────────────
    warn_path = ARTIFACTS_ROOT / "fact_warn_events.parquet"
    warn_df = _safe_read(warn_path)
    if warn_df is not None and len(warn_df) > 0:
        lines = [
            f"WARN Act Layoff Events Summary:",
            f"Total events: {len(warn_df):,}",
            f"Columns: {', '.join(warn_df.columns.tolist())}",
            f"The WARN Act requires 60-day advance notice of mass layoffs.",
            f"Use: Identify employers with recent layoffs that may affect sponsorship.",
        ]
        emp_col = next((c for c in warn_df.columns if c in ("employer_name", "company_name",
                                                             "company", "employer")), None)
        workers_col = next((c for c in warn_df.columns if c in ("num_affected", "workers_affected",
                                                                 "employees_affected", "layoffs")), None)
        if emp_col:
            lines.append(f"\nEmployers with WARN events: {warn_df[emp_col].nunique():,}")
            if workers_col:
                top = warn_df.nlargest(15, workers_col)
                lines.append(f"\nTop 15 largest layoff events:")
                for _, row in top.iterrows():
                    name = row[emp_col]
                    affected = row[workers_col]
                    lines.append(f"  {name}: {affected:,.0f} workers")
        chunks.append(_make_chunk("fact_warn_events.parquet",
                                  "warn_events_summary", "general",
                                  "\n".join(lines)))

    # ── DHS admissions ───────────────────────────────────────────────────
    dhs_path = ARTIFACTS_ROOT / "fact_dhs_admissions.parquet"
    dhs_df = _safe_read(dhs_path)
    if dhs_df is not None and len(dhs_df) > 0:
        lines = [
            f"DHS Admissions Summary:",
            f"Total records: {len(dhs_df):,}",
            f"Columns: {', '.join(dhs_df.columns.tolist())}",
            f"Coverage: Historical DHS admission counts (FY1980+)",
            f"Use: Long-term immigration volume trends.",
        ]
        for _, row in dhs_df.iterrows():
            parts = [str(row.get(c, "")) for c in dhs_df.columns[:4] if pd.notna(row.get(c))]
            if parts:
                lines.append(f"  {' | '.join(parts)}")
        chunks.append(_make_chunk("fact_dhs_admissions.parquet",
                                  "dhs_admissions_summary", "general",
                                  "\n".join(lines)))

    # ── Immigration FAQ chunk ────────────────────────────────────────────
    faq_text = (
        "Frequently Asked Questions about U.S. Employment-Based Immigration:\n\n"
        "Q: What is PERM labor certification?\n"
        "A: PERM (Program Electronic Review Management) is the DOL process where "
        "employers prove no qualified U.S. workers are available for a position. "
        "It is the first step in most EB-2 and EB-3 green card processes.\n\n"
        "Q: What are EB-1, EB-2, and EB-3 categories?\n"
        "A: Employment-based preference categories: EB-1 (extraordinary ability, "
        "outstanding professors, multinational managers), EB-2 (advanced degree or "
        "exceptional ability), EB-3 (skilled workers, professionals, other workers).\n\n"
        "Q: What is a priority date?\n"
        "A: The date when your employer files the PERM application (or I-140 petition "
        "for categories not requiring PERM). It determines your place in the visa queue.\n\n"
        "Q: What is retrogression?\n"
        "A: When the visa bulletin cutoff date moves backward, meaning previously current "
        "applicants can no longer file I-485. Happens when demand exceeds supply.\n\n"
        "Q: What is the H-1B visa?\n"
        "A: A nonimmigrant visa for specialty occupation workers. Subject to annual cap "
        "of 65,000 + 20,000 for U.S. advanced degree holders. Valid for 3 years, extendable."
    )
    chunks.append(_make_chunk("general", "immigration_faq", "general", faq_text))


# ---------------------------------------------------------------------------
# Main builder
# ---------------------------------------------------------------------------

def build_rag_artifacts() -> dict:
    """Build all RAG artifacts and write to artifacts/rag/.

    Returns:
        Summary dict with counts and paths.
    """
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)

    # 1) Build catalog
    print("[RAG] Building artifact catalog...")
    catalog = {
        "program": "NorthStar",
        "project": "Meridian (P2)",
        "description": (
            "Immigration data intelligence platform. Meridian curates U.S. immigration "
            "data from DOL, DOS, USCIS, DHS, and BLS into structured tables, engineered "
            "features, and predictive models. This catalog describes all available data "
            "for answering immigration-related questions."
        ),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "topics": list(TOPICS.keys()),
        "topic_descriptions": {
            "pd_forecast": "Priority date forecasts — when will my green card become current?",
            "employer": "Employer friendliness scores, approval rates, sponsorship history, H-1B data",
            "salary": "Wage benchmarks by occupation and geography (OEWS percentiles), BLS employment stats",
            "visa_bulletin": "Visa bulletin history, cutoff dates, category movement, waiting list",
            "geographic": "Geographic sponsorship patterns — states, cities, metro areas",
            "occupation": "Occupation demand by SOC code — filing volumes, trends, SOC directory",
            "processing": "USCIS processing times, approval/denial statistics",
            "visa_demand": "Visa issuance and application volume — NIV, IV by post, by country",
            "filings": "DOL filing data — LCA (H-1B) and PERM (green card) applications",
            "general": "Country lookups, DHS admissions, WARN layoffs",
        },
        "artifacts": _build_catalog_chunk(),
    }

    catalog_path = RAG_ROOT / "catalog.json"
    catalog_path.write_text(json.dumps(catalog, indent=2, default=str))
    print(f"  → {catalog_path} ({len(catalog['artifacts'])} artifacts)")

    # 2) Build chunks
    print("[RAG] Building text chunks...")
    chunks: list[dict] = []

    _build_pd_forecast_chunks(chunks)
    _build_employer_chunks(chunks)
    _build_salary_chunks(chunks)
    _build_geo_chunks(chunks)
    _build_occupation_chunks(chunks)
    _build_processing_chunks(chunks)
    _build_visa_bulletin_chunks(chunks)
    _build_movement_chunks(chunks)
    _build_backlog_chunks(chunks)
    _build_visa_demand_chunks(chunks)
    _build_iv_post_chunks(chunks)
    _build_lca_perm_chunks(chunks)
    _build_employer_extended_chunks(chunks)
    _build_niv_issuance_chunks(chunks)
    _build_iv_issuance_detail_chunks(chunks)
    _build_oews_detail_chunks(chunks)
    _build_dim_extended_chunks(chunks)
    _build_queue_depth_chunks(chunks)
    _build_h1b_hub_chunks(chunks)
    _build_small_table_chunks(chunks)
    _build_dimension_chunks(chunks)

    # Write chunks by topic
    topic_counts: dict[str, int] = {}
    for topic in TOPICS:
        topic_chunks = [c for c in chunks if c["topic"] == topic]
        if not topic_chunks:
            continue
        topic_counts[topic] = len(topic_chunks)
        out_path = CHUNKS_DIR / f"{topic}.json"
        out_path.write_text(json.dumps(topic_chunks, indent=2, default=str))

    print(f"  → {len(chunks)} chunks across {len(topic_counts)} topics")
    for topic, count in sorted(topic_counts.items()):
        print(f"    {topic}: {count} chunks")

    # 3) Write combined chunks file (for simple full-scan retrieval)
    all_chunks_path = RAG_ROOT / "all_chunks.json"
    all_chunks_path.write_text(json.dumps(chunks, indent=2, default=str))
    print(f"  → {all_chunks_path} ({len(chunks)} total chunks)")

    summary = {
        "catalog_path": str(catalog_path),
        "chunks_dir": str(CHUNKS_DIR),
        "total_chunks": len(chunks),
        "topics": topic_counts,
        "all_chunks_path": str(all_chunks_path),
    }

    # Write summary
    summary_path = RAG_ROOT / "build_summary.json"
    summary_path.write_text(json.dumps(summary, indent=2))
    print(f"  → {summary_path}")

    return summary


if __name__ == "__main__":
    print("=" * 60)
    print("NorthStar Meridian — RAG Builder")
    print("=" * 60)
    result = build_rag_artifacts()
    print(f"\nDone. {result['total_chunks']} chunks generated.")
