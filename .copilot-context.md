# Immigration Model Builder - Project Context

> **âš ï¸ THIS FILE IS SUPPLEMENTARY.** The authoritative context file is [`.github/copilot-instructions.md`](.github/copilot-instructions.md), which Copilot auto-reads at session start. Also read `PROGRESS.md` and `artifacts/metrics/FINAL_SINGLE_REPORT.md`.
>
> Last state: **Milestone 10** complete â€” NorthStar restructure + incremental change detection.

## Last Updated
2026-02-24 (Milestone 10)

## Project Status
ðŸŸ¢ **Milestone 10: NorthStar Restructure & Incremental Builds** â€” 99.7% pass rate, 349 tests, 344 passed / 1 skipped / 3 deselected

### Quick Current State
- **Project root**: `/Users/vrathod1/dev/NorthStar/immigration-model-builder`
- **P1 data**: `/Users/vrathod1/dev/NorthStar/fetch-immigration-data/downloads`
- **Python**: 3.12 (system, no venv)
- **Artifacts**: ~40 parquet files, 5 partitioned dirs, dim_employer 227K rows, fact_perm 1.67M rows
- **All 7 P3 features have complete P2 backing** (verified by P2 readiness audit)
- **Incremental builds**: `bash scripts/build_incremental.sh` â€” detects P1 changes, rebuilds affected artifacts
- **Known issues**: 5 "Unable to Fix" items (see FINAL_SINGLE_REPORT.md)

---

## 3-Project Architecture

### Overview
Building a system that turns public U.S. immigration data into personalized, defensible insights for a public-facing website.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project 1: fetch-immigration-data (LOCAL ONLY)             â”‚
â”‚  Role: Raw data downloader & snapshotter                    â”‚
â”‚  Output: /Users/vrathod1/dev/NorthStar/fetch-immigration-data/downloadsâ”‚
â”‚  Content: Visa Bulletins, PERM, LCA, OEWS, USCIS stats, etc.â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ reads from
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project 2: immigration-model-builder (THIS REPO)           â”‚
â”‚  Role: Parse, normalize, engineer features, train models    â”‚
â”‚  Output: ./artifacts/ (curated tables + model artifacts)    â”‚
â”‚  Run: Manually, locally, after P1 completes                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ exports artifacts to
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project 3: immigration-insights-app (PUBLIC WEB)           â”‚
â”‚  Role: Public website/API serving precomputed insights      â”‚
â”‚  Source: Loads P2 artifacts (NO parsing, NO training)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Critical Paths

### Input (P1 Output - Read Only)
```
/Users/vrathod1/dev/NorthStar/fetch-immigration-data/downloads
```
**DO NOT COPY FILES.** Read directly via configured path.

### Output (P2 Artifacts)
```
./artifacts/
â”œâ”€â”€ tables/    # Curated parquet/csv files
â”œâ”€â”€ models/    # Model artifacts (JSON, pickle, etc.)
â””â”€â”€ metrics/   # DQ metrics and validation reports
```

### Configuration
```
configs/paths.yaml         # data_root + artifacts_root
configs/schemas.yml        # Canonical table schemas (TODO: define)
configs/categories.yml     # EB categories, country codes, mappings
```

---

## Tech Stack
- **Python**: 3.11+
- **Core Dependencies**: pandas, pyarrow, pydantic, pyyaml, pytest, pdfplumber
- **Execution Model**: CLI entrypoints, manual execution, no automation
- **Data Format**: Parquet for facts/dims, JSON for models

---

## P2 Canonical Outputs (Target Schema)

### Dimensions
- `dim_country` - ISO codes, names, regions
- `dim_visa_class` - EB families + subcategories (E11/E12/NIW â†’ EB1/EB2/EB3)
- `dim_soc` - SOC 2018 normalized with crosswalks
- `dim_area` - Area codes aligned to OEWS
- `dim_employer` - Normalized employer names + alias handling

### Facts
- `fact_cutoffs` - Visa Bulletin history (month, chart, category, country, cutoff_date)
- `fact_oews` - BLS wage percentiles (SOC Ã— area Ã— year)
- `fact_perm` - DOL PERM cases (decisions, audits, employers, wages, dates)
- `fact_lca` - DOL LCA filings (intents, wages, SOC, sites, wage_level)
- `fact_uscis_forms` - I-140/I-485 volumes & outcomes by period
- `fact_processing_times` - Per form/category/office time ranges
- `fact_waiting_list` - DOS annual backlog by category Ã— country
- `fact_iv_issuances_monthly` - DOS monthly issuances by country/post

### Features
- `employer_features` - Approval rates, audit rates, wage context, volume (12/24m windows)
- `salary_benchmarks` - Median & P75 by SOC Ã— area (latest OEWS)

### Models
- `pd_forecast` - Priority date forecast with P10/P50/P90 horizons
- `retrogression_risk` - Near-term negative movement classifier [placeholder]
- `employer_friendliness_scores` - 0-100 composite score
- `recommendation_rules` - YAML rules for EB2 vs EB3, PERM timing, employer switch signals

---

## Implementation Status

### âœ… COMPLETE
1. **Repository scaffold** - All directories and stub modules created
2. **Configuration** - paths.yaml pointing to P1 data root
3. **CLI Entrypoints** - All 3 pipelines runnable end-to-end:
   - `python3 -m src.curate.run_curate --paths configs/paths.yaml`
   - `python3 -m src.features.run_features --paths configs/paths.yaml`
   - `python3 -m src.models.run_models --paths configs/paths.yaml`
4. **Build Script** - `bash scripts/build_all.sh` runs full pipeline
5. **Smoke Tests** - 4/4 passing in tests/test_smoke.py
6. **dim_country Dimension** - âœ… FULLY IMPLEMENTED:
   - Schema defined in configs/schemas.yml
   - Builder: src/curate/build_dim_country.py
   - Integrated into run_curate.py
   - Tests: tests/test_dim_country.py (2/2 passing)
   - Output: artifacts/tables/dim_country.parquet (5 countries)
   - Provenance tracking (source_file + ingested_at UTC)
7. **Path Validation Tool** - âœ… IMPLEMENTED:
   - CLI tool: src/io/check_paths.py
   - Validates data_root exists, creates artifacts_root if needed
   - Tests: tests/test_paths_check.py (2/2 passing)
   - Usage: `python -m src.io.check_paths --paths configs/paths.yaml`
8. **fact_cutoffs Table** - âœ… IMPLEMENTED:
   - Schema: 10 fields (bulletin_year, bulletin_month, chart, category, country, cutoff_date, status_flag, source_file, page_ref, ingested_at)
   - Primary key: [bulletin_year, bulletin_month, chart, category, country]
   - Foreign key: country â†’ dim_country.iso3
   - Parser: src/curate/visa_bulletin_loader.py (375 lines, text-based PDF parsing)
   - Integrated into run_curate.py
   - Tests: tests/test_fact_cutoffs.py (3/3 passing)
   - Output: artifacts/tables/fact_cutoffs/year={year}/month={month}/data.parquet (50 rows from 5 PDFs)
   - Chart types: FAD (Final Action Dates), DFF (Dates for Filing)
   - Status flags: C=current, U=unavailable, D=date available
   - Library: pdfplumber>=0.11.0 for PDF parsing
9. **dim_soc Dimension** - âœ… IMPLEMENTED:
   - Schema: 12 fields (soc_code, soc_title, soc_version, soc_major_group, soc_minor_group, soc_broad_group, from_version, from_code, mapping_confidence, is_aggregated, source_file, ingested_at)
   - Primary key: soc_code (SOC 2018 format XX-XXXX)
   - Layout registry: configs/layouts/soc.yml with header aliases and extraction rules
   - Builder: src/curate/build_dim_soc.py (325 lines, adaptive parsing)
   - Integrated into run_curate.py as [2/2] dimension
   - Tests: tests/test_dim_soc.py (4/4 passing)
   - Output: artifacts/tables/dim_soc.parquet (2 SOC codes from crosswalk)
   - Mapping confidence: deterministic, one-to-many, many-to-one, manual-review
   - Graceful degradation: missing titles use "SOC {code} (Title Unknown)"
   - Logs warnings to artifacts/metrics/dim_soc_warnings.log

### ðŸŸ¡ IN PROGRESS
- None currently

### ðŸ“‹ TODO (Next Steps)
1. **Expand fact_cutoffs coverage**:
   - Process more than 5 PDFs (currently limited to MVP subset)
   - Add DFF (Dates for Filing) chart extraction (currently FAD only)
   - Handle "Other Workers" category parsing
   - Add EB5 "Set Aside" sub-categories (Rural, High Unemployment, Infrastructure)

2. **Define Remaining Dimension Schemas** in configs/schemas.yml:
   - dim_visa_class (EB categories mapping)
   - dim_soc (SOC 2018 with crosswalks)
   - dim_area (OEWS area codes)
   - dim_employer (normalized names + aliases)

3. **Implement Additional Dimensions**:
   - Build dim_visa_class (from eb_subcategory_codes.csv)
   - Build dim_soc (from soc_crosswalk_2010_to_2018.csv)

4. **Implement Remaining Fact Loaders**:
   - **PERM** â†’ fact_perm (parse Excel, apply record layouts)
   - **OEWS** â†’ fact_oews (extract wage percentiles by SOC Ã— area)
   - fact_perm (PERM cases)
   - fact_oews (wage data)

3. **Implement Features**:
   - employer_features (from fact_perm)
   - salary_benchmarks (from fact_perm + fact_oews)

4. **Build Baseline PD Forecast Model** (simple statistical baseline)

5. **Add DQ Checks** (row counts, required columns, key coverage, join validation)

6. **Package Artifacts** for P3 consumption with manifest

---

## Key Constraints & Decisions

### Must Remember
- âœ… P2 reads P1 data **directly** (no copying)
- âœ… Manual execution only (no schedulers, watchers, automation)
- âœ… Local-only (no cloud, no APIs in P2)
- âœ… Deterministic & reproducible (track provenance: source files, dates, versions)
- âœ… Modular code with clear separation: curate â†’ features â†’ models â†’ export
- âœ… Start with stubs, iterate incrementally
- âœ… Keep logs clear and informative

### Data Rules
- **Provenance Tracking**: Every curated table should track source file(s), year, layout version
- **Schema Enforcement**: Use pydantic for validation where appropriate
- **Idempotency**: Rerunning pipelines should produce identical results (given same inputs)
- **No Network in P2**: All data comes from P1's local downloads

---

## Module Organization

```
src/
â”œâ”€â”€ io/                  # Data reading utilities
â”‚   â””â”€â”€ readers.py      # load_paths_config, resolve paths, list files
â”‚
â”œâ”€â”€ normalize/          # Normalization & mapping logic
â”‚   â””â”€â”€ mappings.py     # SOC crosswalks, employer normalization, category mapping
â”‚
â”œâ”€â”€ curate/             # Raw â†’ Canonical parsers
â”‚   â”œâ”€â”€ visa_bulletin_loader.py   # PDF â†’ fact_cutoffs
â”‚   â”œâ”€â”€ perm_loader.py            # Excel â†’ fact_perm
â”‚   â”œâ”€â”€ oews_loader.py            # Excel â†’ fact_oews
â”‚   â””â”€â”€ run_curate.py             # CLI entrypoint
â”‚
â”œâ”€â”€ features/           # Feature engineering
â”‚   â”œâ”€â”€ employer_features.py      # Aggregate PERM by employer
â”‚   â”œâ”€â”€ salary_benchmarks.py      # PERM wages + OEWS
â”‚   â””â”€â”€ run_features.py           # CLI entrypoint
â”‚
â”œâ”€â”€ models/             # Model training
â”‚   â”œâ”€â”€ pd_forecast.py            # Time-series forecast
â”‚   â”œâ”€â”€ employer_score.py         # Composite scoring
â”‚   â””â”€â”€ run_models.py             # CLI entrypoint
â”‚)
- âœ… test_smoke.py (4 tests) - Config, paths, imports, CLI execution
- âœ… test_dim_country.py (2 tests) - dim_country builder and schema validation
- **Total: 6/6 tests passing**tifact packaging
    â””â”€â”€ package_artifacts.py      # Bundle artifacts with manifest
```

---

## Testing Strategy

### Current Tests (tests/)
- âœ… test_smoke.py (4 tests) - Config, paths, imports, CLI execution
- âœ… test_dim_country.py (2 tests) - dim_country builder and schema validation
- âœ… test_paths_check.py (2 tests) - Path validation CLI tool
- **Total: 8/8 tests passing**

### Future Tests (TODO)
- Schema validation (all required fields present)
- Data quality thresholds (row counts, null rates, key uniqueness)
- Join integrity (FK relationships)
- Regression tests (known input â†’ expected output)

---

## Development Workflow

```bash
# 1. Activate environment
python3 -m venv .venv && source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run individual stages
python3 -m src.curate.run_curate --paths configs/paths.yaml
python3 -m src.features.run_features --paths configs/paths.yaml
python3 -m src.models.run_models --paths configs/paths.yaml

# 4. Run full pipeline
bash scripts/build_all.sh

# 5. Run tests
pytest -q
# or
bash scripts/run_tests.sh
```

---

## Long-Term Vision (Final User Features)

### Personalized Insights
- **Green Card Forecast**: Expected wait time, retrogression probability, projection to "current"
- **Employer Insights**: GC friendliness score, audit risk, denial trends, wage comparison, layoff risk
- **Job Market Insights**: Where similar roles get sponsored, best employers per occupation, salary competitiveness
- **Recommendations**: EB2 vs EB3 guidance, employer switch signals, PERM timing advice

### Visual Dashboards (P3)
- Employer comparisons
- Bulletin movement timelines
- Wage heat maps
- Risk gauges
- Timeline projections

---

## Session Handhema definition & implementation in progress
- **Completed**: dim_country dimension fully implemented
- **Next Action**: Define and implement remaining dimensions (dim_visa_class, dim_soc, etc.)
- **Blockers**: None
- **Dependencies**: P1 data available at configured path
- **Tests**: 6/6 passing (4 smoke + 2 dim_country)architecture
2. Review [`PROGRESS.md`](PROGRESS.md) to see chronological work history
3. Check **Implementation Status** section for current phase
4. Review **TODO** section for next steps
5. Verify paths and run smoke tests if needed
6. Update this file AND PROGRESS.md after major progress

### Current State Summary
- **Phase**: Core dimensions complete, ready for fact tables requiring SOC joins
- **Completed**: dim_country (5 rows), dim_soc (2 rows), fact_cutoffs (50 rows from 5 PDFs), path validation tool
- **Infrastructure**: Layout registry system in configs/layouts/ for adaptive parsing
- **Libraries**: Added pdfplumber for PDF parsing
- **Next Action**: Implement dim_area and dim_visa_class, then fact_perm (requires dim_soc)
- **Blockers**: None
- **Dependencies**: P1 data available at configured path
- **Tests**: 14/15 passing (4 smoke + 2 dim_country + 4 dim_soc + 3 fact_cutoffs + 2 path checker - 1 pre-existing import failure)

---

## Questions to Ask if Context Unclear
1. What phase are we in? (scaffold / schema / implementation / validation / export)
2. Which loaders are implemented vs stubbed?
3. Are schemas defined in configs/schemas.yml?
4. What does the latest test run show?
5. Are there any known issues or blockers?

---

**Last Session Context**: Implemented dim_soc (SOC 2018 occupation dimension) following adaptive parsing rules. Created configs/layouts/soc.yml with header aliases and extraction rules. Built src/curate/build_dim_soc.py (325 lines) with graceful degradation for missing data. Extracted 2 SOC codes from crosswalk with hierarchy (major/minor/broad), mapping confidence classification, and provenance tracking. Created 4 comprehensive tests (all passing). Established layout registry pattern for schema drift handling. 14/15 tests passing overall. Ready to implement dim_area, dim_visa_class, then fact_perm.
